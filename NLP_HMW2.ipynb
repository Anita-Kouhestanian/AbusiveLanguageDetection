{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_HMW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFMh3Mf0QV8l",
        "colab_type": "code",
        "outputId": "075fa88b-f866-41f8-df5d-a16cf1645365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import os\n",
        "from sklearn.metrics import f1_score\n",
        "import random\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyW-ITB1QhiF",
        "colab_type": "text"
      },
      "source": [
        "# Importing dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1rLiE5AQynJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_folder = '/content/drive/My Drive/NLP_HMW2'\n",
        "train_data = pd.read_csv(\"/content/drive/My Drive/NLP_HMW2/train.csv\", encoding=\"ISO-8859-1\")\n",
        "dev_data = pd.read_csv(\"/content/drive/My Drive/NLP_HMW2/dev.csv\", encoding=\"ISO-8859-1\")\n",
        "test_data = pd.read_csv(\"/content/drive/My Drive/NLP_HMW2/test_no_label.csv\", encoding=\"ISO-8859-1\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uix2lu34V07R",
        "colab_type": "code",
        "outputId": "dadda1f1-2cc8-4cd2-f55c-d6b8e482bfaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_set = [val for key, val in train_data.items()]\n",
        "dev_set = [val for key, val in dev_data.items()]\n",
        "test_set = [val for key, val in test_data.items()]\n",
        "\n",
        "len(train_set), len(dev_set), len(test_set)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 3, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNbQ8Yw1tnPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(train_data)):\n",
        "    if len(train_set[1][i]) < 1000:\n",
        "        print('ID: {} \\nComment: {} \\nLabel: {}\\n'.format(i, train_set[1][i], train_set[2][i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktMs_jFuW4fZ",
        "colab_type": "text"
      },
      "source": [
        "# Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Wm7EslOW_cZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "\n",
        "def tokenization(data):\n",
        "    for i in range(len(data[1])):\n",
        "        tokens = tokenizer.tokenize(data[1][i].lower())\n",
        "        data[1][i] = tokens\n",
        "    return data\n",
        "\n",
        "\n",
        "train_set = tokenization(train_set)\n",
        "dev_set = tokenization(dev_set)\n",
        "test_set = tokenization(test_set)\n",
        "\n",
        "train_token_list = []\n",
        "dev_token_list = []\n",
        "test_token_list = []\n",
        "for i in range(len(train_set[1])):\n",
        "    train_token_list.append(train_set[1][i])\n",
        "for i in range(len(dev_set[1])):\n",
        "    dev_token_list.append(dev_set[1][i])\n",
        "for i in range(len(test_set[1])):\n",
        "    test_token_list.append(test_set[1][i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu9uS0c-3R1i",
        "colab_type": "code",
        "outputId": "afa7d605-b5ac-41bb-a5e4-0882eede3d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_set_=list(zip(train_token_list, train_set[2]))\n",
        "train_set_[2]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['now',\n",
              "  'question',\n",
              "  'is',\n",
              "  ',',\n",
              "  'pakistan',\n",
              "  'will',\n",
              "  'adhere',\n",
              "  'to',\n",
              "  'this',\n",
              "  '?'],\n",
              " 'OAG')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXlvEZuhC9vy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_set_=list(zip(dev_token_list, dev_set[2]))\n",
        "for (token,label) in dev_set_:\n",
        "    print(token,label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dGjYi31XNlK",
        "colab_type": "text"
      },
      "source": [
        "# Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUsJZp7aXUZS",
        "colab_type": "code",
        "outputId": "65718573-17ce-428c-87f5-08d747bf16e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "vocabulary = set([t for tokens in train_token_list + dev_token_list for t in tokens])\n",
        "print('Vocabulary size: {}'.format(len(vocabulary)))\n",
        "vocab2idx = {w: i+2 for i, w in enumerate(vocabulary)}\n",
        "idx2vocab = {i+2: w for i, w in enumerate(vocabulary)}\n",
        "\n",
        "vocab2idx['<PAD>'] = 0\n",
        "vocab2idx['<UNK>'] = 1\n",
        "\n",
        "idx2vocab[0] = '<PAD>'\n",
        "idx2vocab[1] = '<UNK>'\n",
        "\n",
        "print('Vocabulary size: {}'.format(len(vocab2idx)))\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 25661\n",
            "Vocabulary size: 25663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i2sq9uPXa2x",
        "colab_type": "text"
      },
      "source": [
        "# Token Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwxuzo0uXehs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordEncoder(nn.Module):\n",
        "    def __init__(self, vocab, seqlen=300, varlen = False):\n",
        "        super(WordEncoder, self).__init__()\n",
        "\n",
        "        self.word2idx = vocab\n",
        "        self.sequence_length = seqlen\n",
        "        self.various_length = varlen\n",
        "\n",
        "    def one_hot(self, token_id):\n",
        "\n",
        "        encoded = np.zeros(len(self.word2idx), dtype=int)\n",
        "        encoded[token_id] = 1.0\n",
        "        return encoded\n",
        "\n",
        "    def forward(self, samples):\n",
        "        pad_idx = self.word2idx['<PAD>']\n",
        "        unk_idx = self.word2idx['<UNK>']\n",
        "\n",
        "        if self.various_length:\n",
        "\n",
        "            # Find the length of the longest sample\n",
        "            maxlen = max([len(s) for s in samples])\n",
        "\n",
        "        else:\n",
        "\n",
        "            # Use a constant length for all samples\n",
        "            maxlen = self.sequence_length\n",
        "\n",
        "        encoded = [[self.word2idx.get(token, unk_idx) for token in tokens] for tokens in samples]\n",
        "\n",
        "        padded = np.zeros((len(samples), maxlen), dtype=int)\n",
        "        masks = torch.zeros(len(samples), maxlen).long()\n",
        "        # padding and Masking\n",
        "        for i in range(len(encoded)):\n",
        "            padded[i, :len(encoded[i])] = np.array(encoded[i])[:maxlen]\n",
        "            masks[i, :len(encoded[i])] = 1\n",
        "\n",
        "        encoded = [[self.one_hot(token) for token in tokens] for tokens in padded]\n",
        "        encoded = torch.tensor(encoded).long()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            encoded = encoded.cuda()\n",
        "            masks = masks.cuda()\n",
        "\n",
        "        result = {\n",
        "            'mask': masks,\n",
        "            'encoded': encoded\n",
        "        }\n",
        "\n",
        "        return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7a-CEVEXsPL",
        "colab_type": "code",
        "outputId": "2946976e-2a6a-4889-d249-35a620df4309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        }
      },
      "source": [
        "encoder1 = WordEncoder(vocab2idx)\n",
        "encoder2 = WordEncoder(vocab2idx, varlen=True)\n",
        "tokens = train_token_list[4984-1:4984+1]\n",
        "encoder_result = encoder1(tokens)\n",
        "\n",
        "print(\"Encoded:\\n{}\".format(encoder_result['encoded']))\n",
        "print(\"Encoding Shape:\\n{}\\n\".format(encoder_result['encoded'].shape))\n",
        "print(\"Mask:\\n{}\".format(encoder_result['mask']))\n",
        "# print(\"Labels:\\n{}\\n\".format(labels))\n",
        "print(\"Tokens:\\n{}\".format(tokens))\n",
        "\n",
        "tokens = train_token_list[4984-1:4984+1]\n",
        "encoder_result = encoder2(tokens)\n",
        "\n",
        "print(\"Encoding Shape:\\n{}\\n\".format(encoder_result['encoded'].shape))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded:\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0]]])\n",
            "Encoding Shape:\n",
            "torch.Size([2, 300, 25663])\n",
            "\n",
            "Mask:\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "Tokens:\n",
            "[['really', '?', \"i'm\", 'not', 'the', 'one', 'dictating', 'an', 'official', 'language', 'all', 'the', 'while', 'degrading', 'others', 'you', 'hypocrite', '.', 'there', 'is', 'a', 'reason', 'why', 'jews', 'have', 'been', 'hated', 'throughout', 'history', 'and', 'this', 'is', 'yet', 'another', 'example', 'of', 'their', 'tyranny', '.'], ['anna', 'hazare', '..', 'take', 'rest', '...', 'stay', 'at', 'home', '...', 'relax', '..']]\n",
            "Encoding Shape:\n",
            "torch.Size([2, 39, 25663])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVszvc6SZA-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordEmbedder(nn.Module):\n",
        "    def __init__(self, vocab, glove_file, seqlen=300, varlen=False):\n",
        "        super(WordEmbedder, self).__init__()\n",
        "        assert os.path.exists(glove_file) and glove_file.endswith('.txt'), glove_file\n",
        "\n",
        "        self.emb_dim = None\n",
        "\n",
        "        self.PAD_TOKEN = '<PAD>'\n",
        "        self.UNK_TOKEN = '<UNK>'\n",
        "        self.sequence_length = seqlen\n",
        "        self.various_length = varlen\n",
        "\n",
        "        idx2word = [self.PAD_TOKEN, self.UNK_TOKEN]\n",
        "        idx2vect = [None, None]\n",
        "\n",
        "        with open(glove_file, 'r', encoding=\"utf8\") as fp:\n",
        "            for line in fp:\n",
        "                line = line.split()\n",
        "\n",
        "                if line[0] not in vocab:\n",
        "                    continue\n",
        "\n",
        "                w = line[0]\n",
        "                v = np.array([float(value) for value in line[1:]])\n",
        "\n",
        "                if self.emb_dim is None:\n",
        "                    self.emb_dim = v.shape[0]\n",
        "\n",
        "                idx2word.append(w)\n",
        "                idx2vect.append(v)\n",
        "\n",
        "        idx2vect[0] = np.zeros(self.emb_dim)\n",
        "        idx2vect[1] = np.mean(idx2vect[2:], axis=0)\n",
        "\n",
        "        self.embeddings = torch.from_numpy(np.array(idx2vect)).float()\n",
        "        self.embeddings = nn.Embedding.from_pretrained(self.embeddings, freeze=False)\n",
        "\n",
        "        self.idx2word = {i: w for i, w in enumerate(idx2word)}\n",
        "        self.word2idx = {w: i for i, w in self.idx2word.items()}\n",
        "\n",
        "    def forward(self, samples):\n",
        "        pad_idx = self.word2idx[self.PAD_TOKEN]\n",
        "        unk_idx = self.word2idx[self.UNK_TOKEN]\n",
        "\n",
        "        if self.various_length:\n",
        "\n",
        "            # Find the length of the longest sample\n",
        "            maxlen = max([len(s) for s in samples])\n",
        "\n",
        "        else:\n",
        "\n",
        "            # Use a constant length for all samples\n",
        "            maxlen = self.sequence_length\n",
        "\n",
        "        encoded = [[self.word2idx.get(token, unk_idx) for token in tokens] for tokens in samples]\n",
        "\n",
        "        padded = np.zeros((len(samples), maxlen), dtype=int)\n",
        "        masks = torch.zeros(len(samples), maxlen).long()\n",
        "         # Padding and masking\n",
        "        for i in range(len(encoded)):\n",
        "            masks[i, :len(encoded[i])] = 1\n",
        "            padded[i, :len(encoded[i])] = np.array(encoded[i])[:maxlen]\n",
        "            # encoded[i] += [pad_idx] * max(0, (maxlen - len(encoded[i])))\n",
        "\n",
        "        encoded = torch.tensor(padded).long()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            encoded = encoded.cuda()\n",
        "            masks = masks.cuda()\n",
        "\n",
        "        result = {\n",
        "            'output': self.embeddings(encoded),\n",
        "            'mask': masks,\n",
        "            'encoded': encoded\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMxZ0zbaee3m",
        "colab_type": "code",
        "outputId": "11d9ba71-2c99-452c-f020-1847c8a359f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "embedder = WordEmbedder(vocabulary, '/content/drive/My Drive/NLP_HMW2/glove.6B.100d.txt', varlen=True)\n",
        "\n",
        "tokens = train_token_list[4984-1:4984+1]\n",
        "embedder_result = embedder(tokens)\n",
        "\n",
        "print(\"Encoded:\\n{}\".format(embedder_result['encoded']))\n",
        "print(\"Embedding Shape:\\n{}\\n\".format(embedder_result['output'].shape))\n",
        "print(\"Tokens:\\n{}\".format(tokens))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded:\n",
            "tensor([[  579,   185,     1,    35,     2,    47, 12371,    28,   235,  1004,\n",
            "            63,     2,   107, 10859,   418,    79, 13673,     4,    62,    15,\n",
            "             9,  1208,   724,  2501,    32,    50,  8164,   958,   294,     7,\n",
            "            36,    15,   545,   167,   858,     5,    43, 10085,     4],\n",
            "        [ 4270, 15341,     1,   187,  1005,   429,  1058,    23,   160,   429,\n",
            "          7268,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
            "Embedding Shape:\n",
            "torch.Size([2, 39, 100])\n",
            "\n",
            "Tokens:\n",
            "[['really', '?', \"i'm\", 'not', 'the', 'one', 'dictating', 'an', 'official', 'language', 'all', 'the', 'while', 'degrading', 'others', 'you', 'hypocrite', '.', 'there', 'is', 'a', 'reason', 'why', 'jews', 'have', 'been', 'hated', 'throughout', 'history', 'and', 'this', 'is', 'yet', 'another', 'example', 'of', 'their', 'tyranny', '.'], ['anna', 'hazare', '..', 'take', 'rest', '...', 'stay', 'at', 'home', '...', 'relax', '..']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0RFpmbcet9p",
        "colab_type": "text"
      },
      "source": [
        "# BiGRU Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnmip87Gesgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiGRULayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(BiGRULayer, self).__init__()\n",
        "        \n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim // 2, bidirectional=True)\n",
        "\n",
        "    def forward(self, vectors, mask):\n",
        "        batch_size = vectors.size(0)\n",
        "        maxlen = vectors.size(1)\n",
        "        lengths = mask.sum(-1)\n",
        "        \n",
        "        gru_out, _ = self.gru(vectors)  # (batch, seq_len, hidden_dim)\n",
        "        \n",
        "        assert gru_out.size(0) == batch_size\n",
        "        assert gru_out.size(1) == maxlen\n",
        "        assert gru_out.size(2) == self.hidden_dim\n",
        "\n",
        "        # Separate the directions of the GRU\n",
        "        gru_out = gru_out.view(batch_size, maxlen, 2, self.hidden_dim // 2)\n",
        "\n",
        "        # Pick up the last hidden state per direction\n",
        "        fw_last_hn = gru_out[range(batch_size), lengths - 1, 0]  # (batch, hidden // 2)\n",
        "        bw_last_hn = gru_out[range(batch_size), 0, 1]            # (batch, hidden // 2)\n",
        "        \n",
        "        last_hn = torch.cat([fw_last_hn, bw_last_hn], dim=1)      # (batch, hidden // 2) -> (batch, hidden)\n",
        "\n",
        "        return {'output': last_hn, 'outputs': gru_out}\n",
        "\n",
        "\n",
        "bigru_layer = BiGRULayer(embedder.emb_dim, 60)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XibcRgGwMk6b",
        "colab_type": "code",
        "outputId": "48edbee6-8bf0-47b0-d824-e9a055d4e69a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "bigru_layer"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiGRULayer(\n",
              "  (gru): GRU(100, 30, bidirectional=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79mvqYZSe4aL",
        "colab_type": "text"
      },
      "source": [
        "# RNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NvgOgAwe3ln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, embedder, extractor, num_class):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.embedder = embedder\n",
        "        self.extractor = extractor\n",
        "        self.classifier = nn.Linear(extractor.hidden_dim, num_class)\n",
        "\n",
        "    def forward(self, tokens, targets=None):\n",
        "        embedded = self.embedder(tokens)\n",
        "        extracted = self.extractor(embedded['output'], embedded['mask'])\n",
        "\n",
        "        output = torch.nn.functional.softmax(self.classifier(extracted['output']), dim=1)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "gru_model = RNNClassifier(embedder, bigru_layer, num_class=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-HVA2befCgT",
        "colab_type": "code",
        "outputId": "c92f7947-e0eb-4011-d231-95b06ad64b25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "dummy_tokens = ['you are stupid'.split(), 'you are nice!'.split(), 'the deeper you dive the shallower cushion you have.'.split()]\n",
        "dummy_labels = torch.tensor([0, 1]).long()\n",
        "result = gru_model(dummy_tokens, dummy_labels)\n",
        "\n",
        "probs = result.detach().cpu().numpy().tolist()\n",
        "preds = result.detach().cpu().numpy().tolist()\n",
        "for i in range(len(preds)):\n",
        "    for j in range(len(preds[i])):\n",
        "        if preds[i][j] == max(preds[i]):\n",
        "            preds[i][j] = 1\n",
        "        else:\n",
        "            preds[i][j] = 0\n",
        "\n",
        "print('predictions: {}\\nprobabilities: {}'.format(preds, probs))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions: [[0, 1, 0], [0, 1, 0], [0, 1, 0]]\n",
            "probabilities: [[0.3167872726917267, 0.36374107003211975, 0.31947168707847595], [0.2993258535861969, 0.3759068548679352, 0.3247673511505127], [0.27169036865234375, 0.38944023847579956, 0.33886945247650146]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpuuVUM2q9X5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def track_best_model(model_path, model, epoch, best_acc, dev_acc, dev_loss):\n",
        "    if best_acc > dev_acc:\n",
        "        return best_acc, ''\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'F1_Score': dev_acc,\n",
        "        'loss': dev_loss,\n",
        "        'model': model.state_dict()\n",
        "    }\n",
        "    torch.save(state, model_path)\n",
        "    \n",
        "    return dev_acc, ' * '"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzl_OjIqfJfc",
        "colab_type": "text"
      },
      "source": [
        "# Training the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAThanV9fNJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, shuffled_train_set, batch_size):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    batch_tokens, batch_target = [], []\n",
        "    random.Random(1234).shuffle(shuffled_train_set)\n",
        "    \n",
        "    for i in range(len(shuffled_train_set)):\n",
        "      (token, label) = shuffled_train_set[i]\n",
        "      batch_tokens.append(token)\n",
        "      if label == 'NAG':\n",
        "        label_ = [1, 0, 0]\n",
        "      elif label == 'CAG':\n",
        "        label_ = [0, 1, 0]\n",
        "      elif label == 'OAG':\n",
        "        label_ = [0, 0, 1]\n",
        "      batch_target.append(label_)\n",
        "      if len(batch_tokens) == batch_size or i == len(shuffled_train_set) - 1:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch_tokens)\n",
        "        y_pred = out.cpu()\n",
        "        # print(len(y_pred))\n",
        "        # print(len(batch_target))\n",
        "        # exit()\n",
        "        loss = F.binary_cross_entropy(y_pred, torch.tensor(batch_target).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batch_tokens, batch_target = [], []\n",
        "    return model, shuffled_train_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e3HKd3OfUGW",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYsU9yQffXGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx2label = {'NAG':0, 'CAG':1, 'OAG':2}\n",
        "def evaluate(model, dev_set_, batch_size):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    batch_tokens, batch_target = [], []\n",
        "    predictions, actual = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(dev_set_)):\n",
        "            (token, label) = dev_set_[i]\n",
        "            batch_tokens.append(token)\n",
        "            if label == 'NAG':\n",
        "                label_ = [1, 0, 0]\n",
        "            elif label == 'CAG':\n",
        "                label_ = [0, 1, 0]\n",
        "            elif label == 'OAG':\n",
        "                label_ = [0, 0, 1]\n",
        "            batch_target.append(label_)\n",
        "            actual.append(idx2label[label])\n",
        "            if len(batch_tokens) == batch_size or i == len(dev_set_) - 1:\n",
        "                  out = model(batch_tokens,batch_target)\n",
        "                  y_pred = out.cpu()\n",
        "                  predictions.extend(list(torch.argmax(y_pred, -1).numpy()))\n",
        "                  # actual.extend(batch_target)\n",
        "                  loss = F.binary_cross_entropy(y_pred, torch.tensor(batch_target).float())\n",
        "                  total_loss += loss.item()\n",
        "                  batch_tokens, batch_target = [], []\n",
        "    \n",
        "    f1_score_ = f1_score(actual, predictions, average='weighted')\n",
        "    return predictions, total_loss / len(dev_set_), f1_score_\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7_hU_TIusy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_loop(config, model_):\n",
        "\n",
        "    model = model_\n",
        "    optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=config['momentum'])\n",
        "\n",
        "    shuffled_train_set = train_set_\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(config['epoch']):\n",
        "\n",
        "        epoch_msg = '[Epoch {}] / {}'.format(epoch+1, config['epoch'])\n",
        "\n",
        "        model, shuffled_train_set = train(model, optimizer, shuffled_train_set, batch_size=32)\n",
        "\n",
        "        train_pred, train_loss, train_acc = evaluate(model, shuffled_train_set, batch_size=128)\n",
        "        epoch_msg += ' [TRAIN] Loss: {:.4f}, Weighted F1_Score: {:.4f}'.format(train_loss, train_acc)\n",
        "        val_pred, val_loss, val_acc = evaluate(model, dev_set_, batch_size=128)\n",
        "        epoch_msg += ' [DEV] Loss: {:.4f}, Weighted F1_Score: {:.4f}'.format(val_loss, val_acc)\n",
        "        # test_pred, test_loss, test_acc = evaluate(model, test_token_list, batch_size=128)\n",
        "        # epoch_msg += ' [TEST] Loss: {:.4f}, Weighted F1_Score: {:.4f}'.format(test_loss, test_acc)\n",
        "\n",
        "        best_acc, epoch_track = track_best_model(config['checkpoint'], model, epoch, best_acc, val_acc, val_loss)\n",
        "        print(epoch_msg + epoch_track)\n",
        "\n",
        "    print('Done Training!')\n",
        "\n",
        "    state = torch.load(config['checkpoint'])\n",
        "    model.load_state_dict(state['model'])\n",
        "\n",
        "    print('Returning best model from epoch {} with loss {:.5f} and Weighted F1_Score {:.5f}'.format(state['epoch'], state['loss'], state['F1_Score']))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNClZxR3feOL",
        "colab_type": "code",
        "outputId": "66965929-d32f-48b2-9688-ab84d0c6a668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "config = {\n",
        "    'lr': 1e-2,\n",
        "    'momentum': 0.99,\n",
        "    'epoch': 10,\n",
        "    'checkpoint': 'rnn_model.pt'\n",
        "}\n",
        "model = training_loop(config, gru_model)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1] / 10 [TRAIN] Loss: 0.0047, Weighted F1_Score: 0.4123 [DEV] Loss: 0.0049, Weighted F1_Score: 0.3783 * \n",
            "[Epoch 2] / 10 [TRAIN] Loss: 0.0046, Weighted F1_Score: 0.4258 [DEV] Loss: 0.0049, Weighted F1_Score: 0.3961 * \n",
            "[Epoch 3] / 10 [TRAIN] Loss: 0.0046, Weighted F1_Score: 0.4700 [DEV] Loss: 0.0048, Weighted F1_Score: 0.4364 * \n",
            "[Epoch 4] / 10 [TRAIN] Loss: 0.0045, Weighted F1_Score: 0.4819 [DEV] Loss: 0.0048, Weighted F1_Score: 0.4262\n",
            "[Epoch 5] / 10 [TRAIN] Loss: 0.0046, Weighted F1_Score: 0.4371 [DEV] Loss: 0.0049, Weighted F1_Score: 0.4043\n",
            "[Epoch 6] / 10 [TRAIN] Loss: 0.0044, Weighted F1_Score: 0.5076 [DEV] Loss: 0.0048, Weighted F1_Score: 0.4470 * \n",
            "[Epoch 7] / 10 [TRAIN] Loss: 0.0044, Weighted F1_Score: 0.5236 [DEV] Loss: 0.0048, Weighted F1_Score: 0.4282\n",
            "[Epoch 8] / 10 [TRAIN] Loss: 0.0044, Weighted F1_Score: 0.4560 [DEV] Loss: 0.0050, Weighted F1_Score: 0.3802\n",
            "[Epoch 9] / 10 [TRAIN] Loss: 0.0042, Weighted F1_Score: 0.5341 [DEV] Loss: 0.0049, Weighted F1_Score: 0.4391\n",
            "[Epoch 10] / 10 [TRAIN] Loss: 0.0042, Weighted F1_Score: 0.5403 [DEV] Loss: 0.0049, Weighted F1_Score: 0.4318\n",
            "Done Training!\n",
            "Returning best model from epoch 5 with loss 0.00481 and Weighted F1_Score 0.44700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPmTXWqy8qFs",
        "colab_type": "code",
        "outputId": "1ec974a2-4953-4eae-cb3d-541fbc276233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "test_labels = torch.tensor([0, 1]).long()\n",
        "result = model(test_token_list, dummy_labels)\n",
        "\n",
        "probs_ = result.detach().cpu().numpy().tolist()\n",
        "preds_ = result.detach().cpu().numpy().tolist()\n",
        "for i in range(len(preds_)):\n",
        "    for j in range(len(preds_[i])):\n",
        "        if preds_[i][j] == max(preds_[i]):\n",
        "            preds_[i][j] = 1\n",
        "        else:\n",
        "            preds_[i][j] = 0\n",
        "\n",
        "print('predictions: {}\\nprobabilities: {}'.format(preds_, probs_))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions: [[1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0]]\n",
            "probabilities: [[0.5102939605712891, 0.34282079339027405, 0.1468852311372757], [0.43667134642601013, 0.44692087173461914, 0.11640789359807968], [0.683434247970581, 0.17940962314605713, 0.13715611398220062], [0.43483611941337585, 0.30577215552330017, 0.25939178466796875], [0.4860127866268158, 0.2851981818675995, 0.22878910601139069], [0.2968936264514923, 0.4105723798274994, 0.2925339341163635], [0.24140863120555878, 0.3861481845378876, 0.37244319915771484], [0.4536953270435333, 0.34336957335472107, 0.20293503999710083], [0.18603719770908356, 0.32311367988586426, 0.4908490777015686], [0.1010398343205452, 0.39512836933135986, 0.5038317441940308], [0.2551858425140381, 0.4587688148021698, 0.2860454022884369], [0.7759813070297241, 0.1889858990907669, 0.03503278270363808], [0.4513263702392578, 0.3532143235206604, 0.19545932114124298], [0.17262805998325348, 0.5411276817321777, 0.28624430298805237], [0.21913662552833557, 0.5281900763511658, 0.2526732087135315], [0.5179803967475891, 0.4126727283000946, 0.06934677809476852], [0.9679301381111145, 0.02599533461034298, 0.006074517499655485], [0.09371552616357803, 0.39928826689720154, 0.5069962739944458], [0.1385074108839035, 0.6305694580078125, 0.230923131108284], [0.20649847388267517, 0.35501590371131897, 0.43848559260368347], [0.2021528035402298, 0.36130163073539734, 0.4365455210208893], [0.0839601382613182, 0.4784237742424011, 0.43761610984802246], [0.7617058157920837, 0.12045453488826752, 0.11783961206674576], [0.37048402428627014, 0.40118467807769775, 0.2283313274383545], [0.8416151404380798, 0.08807503432035446, 0.07030981779098511], [0.6070424318313599, 0.26509883999824524, 0.12785878777503967], [0.17253585159778595, 0.3906487822532654, 0.43681538105010986], [0.35982125997543335, 0.34449291229248047, 0.2956858277320862], [0.5523200631141663, 0.3076629042625427, 0.14001701772212982], [0.7868607640266418, 0.14260528981685638, 0.07053396105766296], [0.3060995042324066, 0.36917173862457275, 0.32472872734069824], [0.051028523594141006, 0.4183484613895416, 0.5306229591369629], [0.1653202772140503, 0.40775537490844727, 0.42692428827285767], [0.2533590495586395, 0.40868180990219116, 0.3379591405391693], [0.5084697008132935, 0.3099876344203949, 0.18154262006282806], [0.28226494789123535, 0.40456950664520264, 0.313165545463562], [0.35891076922416687, 0.5510326027870178, 0.09005661308765411], [0.6292141675949097, 0.29062971472740173, 0.0801561251282692], [0.719185471534729, 0.22545753419399261, 0.05535699427127838], [0.2877771854400635, 0.43857699632644653, 0.2736457884311676], [0.47757837176322937, 0.3184053897857666, 0.20401620864868164], [0.6577766537666321, 0.21041740477085114, 0.13180586695671082], [0.23660005629062653, 0.427933007478714, 0.3354669213294983], [0.3119412958621979, 0.49320104718208313, 0.1948576122522354], [0.44742393493652344, 0.2793799638748169, 0.2731960713863373], [0.6061103343963623, 0.24002642929553986, 0.15386325120925903], [0.8239246010780334, 0.13106176257133484, 0.04501359537243843], [0.6321889758110046, 0.25907590985298157, 0.10873506963253021], [0.1523190587759018, 0.3917297124862671, 0.4559512138366699], [0.7757492065429688, 0.18746909499168396, 0.036781709641218185], [0.25086212158203125, 0.38565877079963684, 0.3634791076183319], [0.6270275712013245, 0.2892986834049225, 0.08367374539375305], [0.7694803476333618, 0.09216038882732391, 0.1383591741323471], [0.2280789613723755, 0.4380504786968231, 0.333870530128479], [0.3720718324184418, 0.39527904987335205, 0.23264917731285095], [0.48313337564468384, 0.28815194964408875, 0.2287147492170334], [0.20290054380893707, 0.25631654262542725, 0.5407828688621521], [0.44221004843711853, 0.35071155428886414, 0.20707838237285614], [0.3710804879665375, 0.4011828899383545, 0.2277366667985916], [0.24313437938690186, 0.41550490260124207, 0.3413606882095337], [0.43075865507125854, 0.3794606626033783, 0.1897806078195572], [0.820565402507782, 0.12436611950397491, 0.055068500339984894], [0.47156471014022827, 0.299129456281662, 0.22930577397346497], [0.5680197477340698, 0.2648962140083313, 0.1670839935541153], [0.5687634348869324, 0.2745186984539032, 0.15671779215335846], [0.45651984214782715, 0.3676339089870453, 0.17584624886512756], [0.5896276831626892, 0.23487931489944458, 0.1754930019378662], [0.4436614215373993, 0.36907798051834106, 0.18726065754890442], [0.441140741109848, 0.25454220175743103, 0.30431702733039856], [0.3755398094654083, 0.38379934430122375, 0.24066080152988434], [0.926567018032074, 0.057196132838726044, 0.01623685285449028], [0.7635434865951538, 0.15532255172729492, 0.08113403618335724], [0.16993361711502075, 0.5089772343635559, 0.3210890293121338], [0.2314857691526413, 0.558792233467102, 0.20972198247909546], [0.1782197803258896, 0.41151663661003113, 0.4102635383605957], [0.6954702734947205, 0.22068475186824799, 0.08384501188993454], [0.7827876210212708, 0.14646893739700317, 0.07074345648288727], [0.4321710169315338, 0.4028944969177246, 0.1649344563484192], [0.8764066100120544, 0.09999456256628036, 0.023598838597536087], [0.6728425621986389, 0.22686098515987396, 0.10029645264148712], [0.44255954027175903, 0.3399619162082672, 0.21747851371765137], [0.5990535616874695, 0.23557139933109283, 0.16537503898143768], [0.4462718069553375, 0.3092510402202606, 0.24447710812091827], [0.3003290891647339, 0.3616196811199188, 0.3380512595176697], [0.8511807322502136, 0.0947326123714447, 0.05408666655421257], [0.43882328271865845, 0.3545953929424286, 0.20658129453659058], [0.4164586365222931, 0.32834309339523315, 0.25519824028015137], [0.5038090348243713, 0.37497466802597046, 0.12121628224849701], [0.8366319537162781, 0.1444699913263321, 0.01889808475971222], [0.43764716386795044, 0.3292001783847809, 0.23315264284610748], [0.5038424730300903, 0.2531198263168335, 0.24303770065307617], [0.15576855838298798, 0.3620283305644989, 0.4822031259536743], [0.7279259562492371, 0.20356935262680054, 0.0685046836733818], [0.22436794638633728, 0.4364805519580841, 0.33915144205093384], [0.42085736989974976, 0.4162895679473877, 0.16285310685634613], [0.24489131569862366, 0.5996875166893005, 0.15542112290859222], [0.40345263481140137, 0.35233503580093384, 0.2442123144865036], [0.32059618830680847, 0.4181499481201172, 0.26125383377075195], [0.5014495849609375, 0.3163600564002991, 0.18219026923179626], [0.19756820797920227, 0.4139309525489807, 0.388500839471817], [0.768757700920105, 0.14928673207759857, 0.08195552974939346], [0.5362623929977417, 0.27472421526908875, 0.18901339173316956], [0.5176599621772766, 0.33347776532173157, 0.14886225759983063], [0.5985592603683472, 0.2771051526069641, 0.1243356466293335], [0.6880360245704651, 0.16101035475730896, 0.15095369517803192], [0.3859579563140869, 0.2263081669807434, 0.3877338469028473], [0.45279213786125183, 0.38985735177993774, 0.15735049545764923], [0.36926913261413574, 0.34040117263793945, 0.2903296649456024], [0.05502760037779808, 0.3875598907470703, 0.5574125051498413], [0.5510426163673401, 0.27985531091690063, 0.16910210251808167], [0.11083764582872391, 0.33050933480262756, 0.5586530566215515], [0.6722792387008667, 0.24360935389995575, 0.08411142975091934], [0.7452707886695862, 0.20781905949115753, 0.04691006615757942], [0.40366825461387634, 0.5123006105422974, 0.08403114974498749], [0.6101140975952148, 0.28704530000686646, 0.10284052044153214], [0.28940942883491516, 0.4670425355434418, 0.24354806542396545], [0.3080557882785797, 0.4690511226654053, 0.22289302945137024], [0.3299337923526764, 0.3995766341686249, 0.27048957347869873], [0.6564193964004517, 0.2781510055065155, 0.06542954593896866], [0.5385199785232544, 0.23160549998283386, 0.22987444698810577], [0.5647199153900146, 0.24491065740585327, 0.19036945700645447], [0.2855698764324188, 0.41466039419174194, 0.29976972937583923], [0.05173977091908455, 0.4198928475379944, 0.5283673405647278], [0.18770328164100647, 0.5166757702827454, 0.29562100768089294], [0.18830673396587372, 0.45666176080703735, 0.35503146052360535], [0.4191228449344635, 0.3277145028114319, 0.2531626224517822], [0.588322103023529, 0.31864556670188904, 0.09303240478038788], [0.14393752813339233, 0.5283520817756653, 0.3277103900909424], [0.6065007448196411, 0.29963722825050354, 0.09386205673217773], [0.5377278923988342, 0.36471542716026306, 0.09755676984786987], [0.39354729652404785, 0.40082046389579773, 0.20563217997550964], [0.2953471541404724, 0.39643749594688416, 0.30821532011032104], [0.27771681547164917, 0.5368304252624512, 0.18545277416706085], [0.29619595408439636, 0.4314935803413391, 0.2723104953765869], [0.9609056115150452, 0.030532855540513992, 0.00856146402657032], [0.21986249089241028, 0.2580814063549042, 0.5220561027526855], [0.46804720163345337, 0.33297544717788696, 0.19897736608982086], [0.37505534291267395, 0.4277225732803345, 0.1972220540046692], [0.8171331286430359, 0.14731550216674805, 0.035551413893699646], [0.6666662693023682, 0.241987407207489, 0.0913463607430458], [0.21505941450595856, 0.41019511222839355, 0.37474551796913147], [0.6748600006103516, 0.25070494413375854, 0.07443497329950333], [0.8334761261940002, 0.13339272141456604, 0.03313116356730461], [0.7265884280204773, 0.16390228271484375, 0.10950936377048492], [0.6711684465408325, 0.2851872146129608, 0.04364439845085144], [0.4652758836746216, 0.3173503875732422, 0.21737374365329742], [0.49955686926841736, 0.3111782371997833, 0.18926486372947693], [0.3700011372566223, 0.46007367968559265, 0.16992518305778503], [0.40003302693367004, 0.3973619043827057, 0.20260506868362427], [0.5879086256027222, 0.2701599597930908, 0.1419314444065094], [0.22123181819915771, 0.41518405079841614, 0.363584041595459], [0.3439673185348511, 0.3525625467300415, 0.30347010493278503], [0.2586057782173157, 0.2885153591632843, 0.4528788626194], [0.30948755145072937, 0.519167423248291, 0.171345055103302], [0.4012857973575592, 0.4111398160457611, 0.1875743567943573], [0.18711113929748535, 0.4247703552246094, 0.38811853528022766], [0.4149203896522522, 0.4543689787387848, 0.1307106465101242], [0.2937617897987366, 0.46198174357414246, 0.24425649642944336], [0.27724456787109375, 0.38128381967544556, 0.3414715826511383], [0.4536743760108948, 0.3603445887565613, 0.18598102033138275], [0.6940567493438721, 0.18283388018608093, 0.12310938537120819], [0.3732464611530304, 0.3161376118659973, 0.3106159269809723], [0.5645080804824829, 0.29878339171409607, 0.1367085725069046], [0.5008430480957031, 0.36727309226989746, 0.1318839192390442], [0.445878803730011, 0.332335501909256, 0.22178567945957184], [0.29488953948020935, 0.45000025629997253, 0.2551101744174957], [0.4182836413383484, 0.4755086600780487, 0.1062077060341835], [0.7896510362625122, 0.16132931411266327, 0.04901963099837303], [0.3549868166446686, 0.3312424123287201, 0.31377074122428894], [0.7015426158905029, 0.23309478163719177, 0.0653625875711441], [0.8874977827072144, 0.09221623837947845, 0.020285965874791145], [0.515055239200592, 0.26076582074165344, 0.2241789996623993], [0.11543218791484833, 0.49070224165916443, 0.39386555552482605], [0.2575506269931793, 0.4712229371070862, 0.2712264060974121], [0.13381002843379974, 0.5023545622825623, 0.3638354241847992], [0.9048843383789062, 0.07895206660032272, 0.016163690015673637], [0.2384611815214157, 0.42198944091796875, 0.33954939246177673], [0.28732776641845703, 0.4095231294631958, 0.30314910411834717], [0.07104530185461044, 0.39480918645858765, 0.5341454744338989], [0.49366456270217896, 0.36826619505882263, 0.1380692571401596], [0.8859499096870422, 0.09159281104803085, 0.022457212209701538], [0.7218891978263855, 0.2032972276210785, 0.07481356710195541], [0.6372948884963989, 0.31854280829429626, 0.04416236653923988], [0.7576937079429626, 0.15917854011058807, 0.0831276923418045], [0.6379396915435791, 0.216093972325325, 0.14596644043922424], [0.240343376994133, 0.4769287407398224, 0.2827279269695282], [0.5951386094093323, 0.26903507113456726, 0.13582628965377808], [0.48528194427490234, 0.3104032576084137, 0.20431485772132874], [0.678774893283844, 0.19376647472381592, 0.12745866179466248], [0.4486466348171234, 0.4153814911842346, 0.13597184419631958], [0.5600419044494629, 0.26699596643447876, 0.17296209931373596], [0.15267664194107056, 0.4138588607311249, 0.43346449732780457], [0.26527294516563416, 0.3537898063659668, 0.3809373080730438], [0.25016123056411743, 0.5584205985069275, 0.1914181411266327], [0.3478730022907257, 0.3532690405845642, 0.2988578975200653], [0.7324029803276062, 0.22452940046787262, 0.043067678809165955], [0.5692973136901855, 0.3104238212108612, 0.12027881294488907], [0.5817419290542603, 0.27872198820114136, 0.1395360827445984], [0.2700980007648468, 0.429127037525177, 0.3007749915122986], [0.5801427960395813, 0.3061756193637848, 0.1136816218495369], [0.8110528588294983, 0.1471645087003708, 0.04178266227245331], [0.4281435012817383, 0.3923068046569824, 0.1795496642589569], [0.3496590554714203, 0.37434256076812744, 0.2759983539581299], [0.3004557490348816, 0.4712877869606018, 0.2282564640045166], [0.100554458796978, 0.4042433500289917, 0.4952021837234497], [0.2469792515039444, 0.48616883158683777, 0.26685187220573425], [0.47231870889663696, 0.41823697090148926, 0.10944435745477676], [0.24148964881896973, 0.27942827343940735, 0.47908204793930054], [0.4590056836605072, 0.3372761905193329, 0.2037181258201599], [0.430646687746048, 0.33115458488464355, 0.23819875717163086], [0.5163100957870483, 0.2967531979084015, 0.1869366616010666], [0.4780709743499756, 0.3054772913455963, 0.2164517492055893], [0.5060058832168579, 0.38970014452934265, 0.10429389774799347], [0.9737962484359741, 0.02089047245681286, 0.0053131780587136745], [0.13854163885116577, 0.4681476056575775, 0.3933107256889343], [0.20193180441856384, 0.47490817308425903, 0.3231600224971771], [0.26331472396850586, 0.2936410903930664, 0.44304415583610535], [0.3251863718032837, 0.376388818025589, 0.29842475056648254], [0.2541825771331787, 0.399483859539032, 0.3463336229324341], [0.3870391845703125, 0.39403969049453735, 0.21892106533050537], [0.3258938193321228, 0.3775862455368042, 0.296519935131073], [0.32298189401626587, 0.4151551425457001, 0.26186299324035645], [0.23619341850280762, 0.43173372745513916, 0.3320728540420532], [0.8377095460891724, 0.1301639825105667, 0.03212643787264824], [0.41253551840782166, 0.39946436882019043, 0.18800011277198792], [0.7987557053565979, 0.15234015882015228, 0.04890412837266922], [0.4644402861595154, 0.3751671612262726, 0.1603926122188568], [0.48880553245544434, 0.31695470213890076, 0.19423969089984894], [0.670866847038269, 0.16765473783016205, 0.1614784598350525], [0.6469160318374634, 0.2611576020717621, 0.09192640334367752], [0.9054951071739197, 0.07112867385149002, 0.02337622456252575], [0.07103817909955978, 0.301973432302475, 0.626988410949707], [0.4338269531726837, 0.3193221688270569, 0.24685095250606537], [0.5528274774551392, 0.3569166958332062, 0.09025585651397705], [0.7138466835021973, 0.20960329473018646, 0.07654999196529388], [0.47336018085479736, 0.3361174166202545, 0.19052237272262573], [0.5220558643341064, 0.21575358510017395, 0.2621905207633972], [0.06147308647632599, 0.26994985342025757, 0.6685770153999329], [0.6062869429588318, 0.31702154874801636, 0.07669150829315186], [0.3390178680419922, 0.35891103744506836, 0.30207115411758423], [0.3519471287727356, 0.2649230360984802, 0.3831298351287842], [0.5270916819572449, 0.3287546634674072, 0.14415359497070312], [0.5653398036956787, 0.30699896812438965, 0.12766121327877045], [0.31035861372947693, 0.4505428075790405, 0.23909856379032135], [0.7864086627960205, 0.16480489075183868, 0.04878643527626991], [0.20603147149085999, 0.4564875066280365, 0.3374810516834259], [0.23533114790916443, 0.5014951229095459, 0.26317378878593445], [0.4897856116294861, 0.33036068081855774, 0.1798536777496338], [0.4938150942325592, 0.25810515880584717, 0.24807968735694885], [0.3039226233959198, 0.3805527091026306, 0.3155246675014496], [0.7867457866668701, 0.16698038578033447, 0.04627376049757004], [0.7672162055969238, 0.20633962750434875, 0.02644406072795391], [0.6344376802444458, 0.25608402490615845, 0.10947833955287933], [0.14899788796901703, 0.36251330375671387, 0.4884888529777527], [0.5289169549942017, 0.3397617042064667, 0.1313212662935257], [0.2983264923095703, 0.41348686814308167, 0.2881866693496704], [0.2539078891277313, 0.29657623171806335, 0.4495158791542053], [0.37249499559402466, 0.36384835839271545, 0.2636566460132599], [0.24278323352336884, 0.41280245780944824, 0.3444143831729889], [0.7992056608200073, 0.1479572355747223, 0.05283714085817337], [0.6166886687278748, 0.23860418796539307, 0.14470717310905457], [0.29333487153053284, 0.5566360950469971, 0.15002910792827606], [0.2871882915496826, 0.37365666031837463, 0.3391549587249756], [0.5771380662918091, 0.21376021206378937, 0.20910166203975677], [0.43050727248191833, 0.35445481538772583, 0.2150379866361618], [0.31965371966362, 0.34926146268844604, 0.33108481764793396], [0.19598951935768127, 0.37543633580207825, 0.42857420444488525], [0.41433295607566833, 0.4036032259464264, 0.18206384778022766], [0.28367024660110474, 0.3260228931903839, 0.39030686020851135], [0.18019305169582367, 0.2998010814189911, 0.5200058817863464], [0.4194670617580414, 0.35588884353637695, 0.22464406490325928], [0.6610492467880249, 0.2304742932319641, 0.10847639292478561], [0.6070606708526611, 0.23245768249034882, 0.16048164665699005], [0.3462294638156891, 0.45885834097862244, 0.1949121505022049], [0.5708025693893433, 0.35211697220802307, 0.07708042860031128], [0.2836405336856842, 0.4460955262184143, 0.2702639102935791], [0.6746902465820312, 0.23868928849697113, 0.08662047237157822], [0.7492726445198059, 0.17797234654426575, 0.07275498658418655], [0.31925174593925476, 0.4324621856212616, 0.24828605353832245], [0.43087831139564514, 0.34504103660583496, 0.22408068180084229], [0.5481334924697876, 0.3068438768386841, 0.14502254128456116], [0.47149336338043213, 0.312505841255188, 0.21600086987018585], [0.3956395983695984, 0.33296820521354675, 0.27139219641685486], [0.4380960464477539, 0.38045647740364075, 0.18144749104976654], [0.11380390077829361, 0.4720439314842224, 0.414152055978775], [0.7236815690994263, 0.19583186507225037, 0.0804864913225174], [0.4692012369632721, 0.29805949330329895, 0.23273923993110657], [0.35775068402290344, 0.3836587071418762, 0.25859057903289795], [0.4983220100402832, 0.37901508808135986, 0.12266290932893753], [0.22832702100276947, 0.5353594422340393, 0.23631353676319122], [0.49324744939804077, 0.33715081214904785, 0.16960176825523376], [0.6796362996101379, 0.2370186448097229, 0.08334510028362274], [0.5685301423072815, 0.30147749185562134, 0.1299922913312912], [0.5894815325737, 0.2685404419898987, 0.14197809994220734], [0.29524925351142883, 0.3138940632343292, 0.39085665345191956], [0.4979495704174042, 0.22917987406253815, 0.27287057042121887], [0.8880752921104431, 0.08187752962112427, 0.03004721738398075], [0.3221419155597687, 0.4042711555957794, 0.2735868990421295], [0.8304076790809631, 0.11963923275470734, 0.04995311051607132], [0.2698943316936493, 0.30389827489852905, 0.42620745301246643], [0.8090214729309082, 0.13665397465229034, 0.05432453379034996], [0.8712561130523682, 0.09335944056510925, 0.03538443148136139], [0.5656947493553162, 0.25859734416007996, 0.1757078617811203], [0.10185611993074417, 0.4027162790298462, 0.49542760848999023], [0.3754597306251526, 0.39230501651763916, 0.23223519325256348], [0.3087121844291687, 0.42547252774238586, 0.26581525802612305], [0.4675482213497162, 0.3204965889453888, 0.211955264210701], [0.5157755613327026, 0.40219801664352417, 0.0820264145731926], [0.26051095128059387, 0.41279324889183044, 0.3266957998275757], [0.23150543868541718, 0.47647926211357117, 0.29201528429985046], [0.283006489276886, 0.42839381098747253, 0.2885996699333191], [0.35021039843559265, 0.3406776785850525, 0.30911192297935486], [0.3490845561027527, 0.37997812032699585, 0.27093738317489624], [0.4209103584289551, 0.3382989764213562, 0.24079062044620514], [0.48191624879837036, 0.39913809299468994, 0.1189456656575203], [0.9669695496559143, 0.02627905271947384, 0.006751400884240866], [0.6977764964103699, 0.22333842515945435, 0.07888510823249817], [0.5146816372871399, 0.28517311811447144, 0.2001451998949051], [0.269148588180542, 0.440959632396698, 0.28989169001579285], [0.08870057016611099, 0.4220394492149353, 0.4892599284648895], [0.4868302047252655, 0.3404732346534729, 0.1726965606212616], [0.3440725803375244, 0.43011242151260376, 0.2258150577545166], [0.2715478539466858, 0.425251841545105, 0.30320030450820923], [0.3350897431373596, 0.34264808893203735, 0.32226210832595825], [0.22627320885658264, 0.462638258934021, 0.311088502407074], [0.6093523502349854, 0.31406161189079285, 0.07658600807189941], [0.6042196750640869, 0.242680162191391, 0.1531001329421997], [0.8363037109375, 0.13810168206691742, 0.025594528764486313], [0.8652792572975159, 0.11394486576318741, 0.020775888115167618], [0.32620248198509216, 0.39585232734680176, 0.27794525027275085], [0.7248391509056091, 0.21606549620628357, 0.059095319360494614], [0.6876693367958069, 0.2166505604982376, 0.0956801027059555], [0.4247530698776245, 0.32764118909835815, 0.24760572612285614], [0.9152308702468872, 0.06914028525352478, 0.015628790482878685], [0.29088008403778076, 0.3565632998943329, 0.35255664587020874], [0.3018873929977417, 0.3773912787437439, 0.3207213282585144], [0.3665999174118042, 0.35415783524513245, 0.27924227714538574], [0.38004517555236816, 0.4029849171638489, 0.21696998178958893], [0.5120760798454285, 0.37240874767303467, 0.11551519483327866], [0.8345198631286621, 0.13908551633358002, 0.026394620537757874], [0.9198248982429504, 0.06118306890130043, 0.01899198442697525], [0.3827281892299652, 0.4279422461986542, 0.18932953476905823], [0.47601091861724854, 0.36081719398498535, 0.1631718873977661], [0.6161381602287292, 0.20945975184440613, 0.17440201342105865], [0.23918543756008148, 0.47343215346336365, 0.2873823940753937], [0.40874141454696655, 0.27338817715644836, 0.31787046790122986], [0.5807177424430847, 0.3576432764530182, 0.06163899973034859], [0.29777073860168457, 0.3861718773841858, 0.31605732440948486], [0.6127980351448059, 0.2593425214290619, 0.1278594434261322], [0.18761476874351501, 0.4155520498752594, 0.3968331515789032], [0.5970829129219055, 0.30019935965538025, 0.10271774977445602], [0.45632675290107727, 0.331345796585083, 0.21232746541500092], [0.25530439615249634, 0.5555254220962524, 0.18917019665241241], [0.7387269139289856, 0.2212587147951126, 0.04001437872648239], [0.32501739263534546, 0.44693800806999207, 0.22804467380046844], [0.2754732072353363, 0.41626420617103577, 0.30826258659362793], [0.6302345395088196, 0.30590906739234924, 0.06385638564825058], [0.21570949256420135, 0.36841920018196106, 0.4158712923526764], [0.5569456219673157, 0.26478034257888794, 0.1782740205526352], [0.20806066691875458, 0.3282634913921356, 0.4636758863925934], [0.48023319244384766, 0.3362773656845093, 0.18348939716815948], [0.41438326239585876, 0.42900919914245605, 0.15660759806632996], [0.4652892053127289, 0.3551161587238312, 0.17959463596343994], [0.10916756838560104, 0.7516962885856628, 0.1391361802816391], [0.22798363864421844, 0.37237468361854553, 0.39964166283607483], [0.11381807923316956, 0.27290645241737366, 0.6132755279541016], [0.4656856954097748, 0.3671097755432129, 0.16720454394817352], [0.40999382734298706, 0.2853999733924866, 0.304606169462204], [0.11203005909919739, 0.331266850233078, 0.5567030906677246], [0.19369906187057495, 0.48148927092552185, 0.32481175661087036], [0.6323117613792419, 0.23509739339351654, 0.13259080052375793], [0.6302160620689392, 0.2483101785182953, 0.1214737594127655], [0.2059861123561859, 0.4197056293487549, 0.37430819869041443], [0.7183250188827515, 0.16075648367404938, 0.12091850489377975], [0.3286036252975464, 0.33277925848960876, 0.3386170566082001], [0.7124386429786682, 0.17939895391464233, 0.10816241800785065], [0.09358030557632446, 0.3099120855331421, 0.5965076088905334], [0.44211772084236145, 0.37087616324424744, 0.1870061755180359], [0.5527567267417908, 0.24009710550308228, 0.20714621245861053], [0.3319688141345978, 0.4404861629009247, 0.22754508256912231], [0.8437618017196655, 0.1421901136636734, 0.014048105105757713], [0.2476039081811905, 0.39574846625328064, 0.3566476106643677], [0.3112626075744629, 0.30619579553604126, 0.3825416564941406], [0.23679548501968384, 0.3943784534931183, 0.3688260018825531], [0.7685660719871521, 0.12664295732975006, 0.10479100048542023], [0.23326918482780457, 0.5472822189331055, 0.21944867074489594], [0.13161025941371918, 0.28090086579322815, 0.5874889492988586], [0.10686899721622467, 0.22281666100025177, 0.670314371585846], [0.5913528203964233, 0.24434851109981537, 0.1642986536026001], [0.13834357261657715, 0.23753613233566284, 0.6241203546524048], [0.4726111590862274, 0.36692705750465393, 0.16046176850795746], [0.4583057761192322, 0.39167091250419617, 0.15002331137657166], [0.11461879312992096, 0.4643484950065613, 0.42103275656700134], [0.619819700717926, 0.2902049422264099, 0.08997534960508347], [0.1354459971189499, 0.34916558861732483, 0.5153884291648865], [0.7930679321289062, 0.15661737322807312, 0.05031462013721466], [0.2352113574743271, 0.48514649271965027, 0.27964213490486145], [0.5808805227279663, 0.2717464864253998, 0.14737297594547272], [0.7317649126052856, 0.20027923583984375, 0.0679558739066124], [0.4955245852470398, 0.33714544773101807, 0.16732996702194214], [0.27377620339393616, 0.42532581090927124, 0.300898015499115], [0.3057854473590851, 0.37099432945251465, 0.3232201635837555], [0.7051611542701721, 0.18079030513763428, 0.11404852569103241], [0.2680482268333435, 0.3545818328857422, 0.37736988067626953], [0.7046935558319092, 0.2131504863500595, 0.0821559727191925], [0.2640588581562042, 0.3320264220237732, 0.4039146900177002], [0.3497529923915863, 0.23542296886444092, 0.41482406854629517], [0.5262866616249084, 0.2245258241891861, 0.24918751418590546], [0.7039476633071899, 0.2383633553981781, 0.05768899992108345], [0.3768157958984375, 0.3438473045825958, 0.2793368697166443], [0.14483337104320526, 0.36184772849082947, 0.4933188557624817], [0.45035481452941895, 0.43678322434425354, 0.11286196857690811], [0.434105783700943, 0.38807547092437744, 0.17781870067119598], [0.4444756805896759, 0.35300150513648987, 0.20252284407615662], [0.25088244676589966, 0.44516700506210327, 0.3039504587650299], [0.13721567392349243, 0.458520770072937, 0.40426361560821533], [0.21744570136070251, 0.3756924271583557, 0.4068618714809418], [0.7718958258628845, 0.14194902777671814, 0.08615510165691376], [0.4016153812408447, 0.4158032536506653, 0.18258139491081238], [0.7350010275840759, 0.14823229610919952, 0.11676666140556335], [0.6628934144973755, 0.27901753783226013, 0.058089036494493484], [0.5103317499160767, 0.3449918329715729, 0.1446763426065445], [0.635485827922821, 0.324838250875473, 0.0396759919822216], [0.5705428719520569, 0.3019130825996399, 0.12754401564598083], [0.9253290891647339, 0.06603623926639557, 0.008634722791612148], [0.4593490660190582, 0.3808920979499817, 0.15975886583328247], [0.14076797664165497, 0.37238678336143494, 0.4868452250957489], [0.8870012760162354, 0.08701944351196289, 0.025979235768318176], [0.7001521587371826, 0.21482427418231964, 0.08502347767353058], [0.21417148411273956, 0.18691030144691467, 0.5989181995391846], [0.8252524137496948, 0.11783330887556076, 0.05691426247358322], [0.5048142075538635, 0.30867624282836914, 0.18650959432125092], [0.5392157435417175, 0.2896464169025421, 0.1711379736661911], [0.3712490200996399, 0.366405725479126, 0.26234525442123413], [0.4062665104866028, 0.36963093280792236, 0.22410255670547485], [0.5806873440742493, 0.2923717200756073, 0.12694089114665985], [0.7670058012008667, 0.16130691766738892, 0.07168728113174438], [0.4143175482749939, 0.349232017993927, 0.2364504486322403], [0.656061589717865, 0.26674190163612366, 0.07719650864601135], [0.45281749963760376, 0.35470542311668396, 0.19247709214687347], [0.4537431001663208, 0.34574759006500244, 0.20050935447216034], [0.3451768755912781, 0.39419135451316833, 0.2606317400932312], [0.7302314043045044, 0.1918158084154129, 0.07795276492834091], [0.34036099910736084, 0.34022149443626404, 0.3194175064563751], [0.4186052680015564, 0.4649028778076172, 0.1164918914437294], [0.22300663590431213, 0.39580678939819336, 0.3811866044998169], [0.44313085079193115, 0.33059370517730713, 0.22627542912960052], [0.25543734431266785, 0.39351001381874084, 0.3510526716709137], [0.024157173931598663, 0.195287823677063, 0.780555009841919], [0.5129562616348267, 0.3156917095184326, 0.1713520586490631], [0.22513140738010406, 0.40955108404159546, 0.36531752347946167], [0.40443554520606995, 0.32411423325538635, 0.2714502513408661], [0.24018308520317078, 0.31522905826568604, 0.4445878565311432], [0.6017621159553528, 0.3358842730522156, 0.062353573739528656], [0.5297597646713257, 0.20327548682689667, 0.26696479320526123], [0.8160134553909302, 0.13252674043178558, 0.051459766924381256], [0.12566189467906952, 0.43829891085624695, 0.4360392093658447], [0.3823500871658325, 0.3408471941947937, 0.27680277824401855], [0.7628983855247498, 0.17855204641819, 0.05854959413409233], [0.6419100761413574, 0.20728737115859985, 0.15080256760120392], [0.32432642579078674, 0.3121470510959625, 0.36352649331092834], [0.08197396993637085, 0.3922485411167145, 0.5257775187492371], [0.5898930430412292, 0.22352536022663116, 0.1865815669298172], [0.3204573690891266, 0.377184122800827, 0.30235856771469116], [0.6016055345535278, 0.2430696338415146, 0.15532487630844116], [0.2720676064491272, 0.5498811602592468, 0.1780511736869812], [0.4114660918712616, 0.4040142595767975, 0.18451964855194092], [0.5192844867706299, 0.4157574772834778, 0.06495797634124756], [0.5169806480407715, 0.2822585701942444, 0.20076075196266174], [0.2734566628932953, 0.52763831615448, 0.19890496134757996], [0.33607369661331177, 0.4579295516014099, 0.20599669218063354], [0.6939185857772827, 0.2650142312049866, 0.04106726869940758], [0.3198806047439575, 0.40122419595718384, 0.27889513969421387], [0.194430872797966, 0.27456435561180115, 0.5310047268867493], [0.4509632885456085, 0.31018441915512085, 0.23885223269462585], [0.596977710723877, 0.25309309363365173, 0.1499291956424713], [0.27739354968070984, 0.28759756684303284, 0.4350088834762573], [0.40784165263175964, 0.36036065220832825, 0.2317977100610733], [0.31462305784225464, 0.3667598068714142, 0.3186171054840088], [0.35417819023132324, 0.3901832699775696, 0.25563845038414], [0.7610594034194946, 0.16722345352172852, 0.07171722501516342], [0.6572321653366089, 0.23740148544311523, 0.10536637902259827], [0.17591367661952972, 0.4300785958766937, 0.39400774240493774], [0.39206966757774353, 0.3440351188182831, 0.26389527320861816], [0.2778169810771942, 0.46150314807891846, 0.2606799006462097], [0.27526554465293884, 0.4589642882347107, 0.2657701373100281], [0.33294156193733215, 0.49972453713417053, 0.16733381152153015], [0.4751567244529724, 0.3269624710083008, 0.1978807896375656], [0.7450401186943054, 0.17935076355934143, 0.07560903578996658], [0.13182349503040314, 0.23750896751880646, 0.6306675672531128], [0.671530544757843, 0.19177910685539246, 0.13669033348560333], [0.37089604139328003, 0.37109965085983276, 0.2580043077468872], [0.643055260181427, 0.251278817653656, 0.1056659147143364], [0.4269177317619324, 0.36106404662132263, 0.212018221616745], [0.5976643562316895, 0.3342619240283966, 0.06807365268468857], [0.5478594303131104, 0.30125465989112854, 0.1508859097957611], [0.9677867293357849, 0.028520114719867706, 0.0036932150833308697], [0.8345438838005066, 0.10888954997062683, 0.056566644459962845], [0.26312166452407837, 0.42960500717163086, 0.3072732985019684], [0.42877715826034546, 0.4441887140274048, 0.12703414261341095], [0.6872259974479675, 0.21802064776420593, 0.09475327283143997], [0.13062261044979095, 0.38795769214630127, 0.48141974210739136], [0.5931422114372253, 0.2661970853805542, 0.14066070318222046], [0.4841212034225464, 0.42889711260795593, 0.08698173612356186], [0.38202449679374695, 0.3209482729434967, 0.2970271706581116], [0.24630537629127502, 0.3297899663448334, 0.4239047169685364], [0.6067198514938354, 0.2536931335926056, 0.13958698511123657], [0.9391525387763977, 0.05291539430618286, 0.007932057604193687], [0.2926596999168396, 0.437428742647171, 0.2699114680290222], [0.3298432230949402, 0.5518742799758911, 0.11828245222568512], [0.49113601446151733, 0.3345402479171753, 0.17432381212711334], [0.2521326541900635, 0.5029487013816833, 0.24491868913173676], [0.4766479730606079, 0.3438692092895508, 0.1794828176498413], [0.2152637094259262, 0.43232473731040955, 0.3524114787578583], [0.24744516611099243, 0.4138500392436981, 0.3387047350406647], [0.11147940903902054, 0.4824196398258209, 0.40610089898109436], [0.6818844676017761, 0.20247966051101685, 0.11563589423894882], [0.35577547550201416, 0.4245445430278778, 0.21968001127243042], [0.17279775440692902, 0.5107925534248352, 0.31640973687171936], [0.3925001919269562, 0.3758547604084015, 0.23164507746696472], [0.4163174629211426, 0.4098874628543854, 0.17379510402679443], [0.16503234207630157, 0.47072407603263855, 0.3642435371875763], [0.2745782732963562, 0.5517652034759521, 0.17365646362304688], [0.8611426949501038, 0.10410371422767639, 0.03475365787744522], [0.5270878076553345, 0.3121577799320221, 0.1607544720172882], [0.7976589798927307, 0.1633685976266861, 0.03897234424948692], [0.3382188081741333, 0.46863898634910583, 0.19314222037792206], [0.4433719515800476, 0.4040309488773346, 0.15259705483913422], [0.15430034697055817, 0.5329474210739136, 0.31275221705436707], [0.836395800113678, 0.08817106485366821, 0.07543323189020157], [0.6846388578414917, 0.24725568294525146, 0.06810545921325684], [0.2699795961380005, 0.44591280817985535, 0.28410762548446655], [0.8206154704093933, 0.15196840465068817, 0.02741609513759613], [0.3157578706741333, 0.39981797337532043, 0.2844241261482239], [0.5569626092910767, 0.2793653607368469, 0.16367197036743164], [0.1734509915113449, 0.37719446420669556, 0.44935452938079834], [0.5308812260627747, 0.3300769329071045, 0.13904191553592682], [0.5659822225570679, 0.2542009651660919, 0.17981678247451782], [0.2757026255130768, 0.4232463836669922, 0.30105093121528625], [0.3493943214416504, 0.3876272141933441, 0.2629784643650055], [0.1253829151391983, 0.4121955633163452, 0.4624215066432953], [0.5460190773010254, 0.26359203457832336, 0.19038887321949005], [0.41597482562065125, 0.4316505491733551, 0.15237459540367126], [0.45197299122810364, 0.3348216712474823, 0.2132052779197693], [0.5459276437759399, 0.31928926706314087, 0.1347830593585968], [0.24289058148860931, 0.37333208322525024, 0.38377729058265686], [0.4745608866214752, 0.27276551723480225, 0.25267353653907776], [0.5604557991027832, 0.24503418803215027, 0.19450993835926056], [0.5681952834129333, 0.32575124502182007, 0.10605347156524658], [0.7513946890830994, 0.20027005672454834, 0.0483352430164814], [0.30331748723983765, 0.38302797079086304, 0.3136546015739441], [0.7700492143630981, 0.168399840593338, 0.06155101954936981], [0.8215662837028503, 0.13289055228233337, 0.04554317519068718], [0.9445834159851074, 0.04597976803779602, 0.009436811320483685], [0.4444538652896881, 0.39343857765197754, 0.16210758686065674], [0.5754328966140747, 0.34699878096580505, 0.077568419277668], [0.23137898743152618, 0.41771042346954346, 0.3509105443954468], [0.2537094056606293, 0.3596034049987793, 0.38668715953826904], [0.2835249602794647, 0.3408595323562622, 0.37561556696891785], [0.3903932571411133, 0.37504756450653076, 0.23455919325351715], [0.5738809108734131, 0.2748333811759949, 0.15128570795059204], [0.33342406153678894, 0.4514070451259613, 0.21516895294189453], [0.3808126151561737, 0.35615256428718567, 0.2630348205566406], [0.13509850203990936, 0.38959255814552307, 0.47530895471572876], [0.08943312615156174, 0.29654163122177124, 0.6140252351760864], [0.6588096022605896, 0.26270920038223267, 0.07848124206066132], [0.34470126032829285, 0.3585636615753174, 0.2967350482940674], [0.2868022322654724, 0.41120561957359314, 0.30199211835861206], [0.13266508281230927, 0.21738895773887634, 0.6499459743499756], [0.406151682138443, 0.4110886752605438, 0.182759627699852], [0.22086410224437714, 0.4665524661540985, 0.31258338689804077], [0.5124067664146423, 0.3528909683227539, 0.13470223546028137], [0.6254943609237671, 0.28051993250846863, 0.09398569911718369], [0.4452986419200897, 0.37442782521247864, 0.18027351796627045], [0.5232512354850769, 0.32681718468666077, 0.14993153512477875], [0.628913164138794, 0.2744632959365845, 0.09662354737520218], [0.27063411474227905, 0.2934531569480896, 0.43591269850730896], [0.15203315019607544, 0.3842504024505615, 0.4637165367603302], [0.34815606474876404, 0.4266626238822937, 0.22518125176429749], [0.6720892786979675, 0.22001376748085022, 0.10789687931537628], [0.3006911873817444, 0.3940988779067993, 0.3052099347114563], [0.17513658106327057, 0.44949448108673096, 0.37536901235580444], [0.6630731821060181, 0.22934243083000183, 0.10758433490991592], [0.0483071506023407, 0.25436148047447205, 0.6973313689231873], [0.7006263732910156, 0.16142131388187408, 0.13795234262943268], [0.49270689487457275, 0.4074176251888275, 0.09987545758485794], [0.5538455843925476, 0.265832781791687, 0.18032163381576538], [0.6078842282295227, 0.29426902532577515, 0.09784676134586334], [0.7013538479804993, 0.2525348961353302, 0.04611126333475113], [0.4278905987739563, 0.37407851219177246, 0.19803085923194885], [0.27327924966812134, 0.40452688932418823, 0.32219380140304565], [0.4114780128002167, 0.35270464420318604, 0.2358173280954361], [0.53596431016922, 0.3256833851337433, 0.13835231959819794], [0.33288514614105225, 0.4519120156764984, 0.21520276367664337], [0.5537139773368835, 0.3181307911872864, 0.12815533578395844], [0.46455103158950806, 0.37756505608558655, 0.1578839123249054], [0.6332279443740845, 0.25931987166404724, 0.10745218396186829], [0.6252422332763672, 0.27208778262138367, 0.10266997665166855], [0.13188129663467407, 0.5626298189163208, 0.3054889142513275], [0.34349632263183594, 0.3856690227985382, 0.27083468437194824], [0.4166535437107086, 0.37720850110054016, 0.20613794028759003], [0.1327298879623413, 0.5110164284706116, 0.3562536835670471], [0.4747616946697235, 0.3077777922153473, 0.21746046841144562], [0.7590572834014893, 0.16449180245399475, 0.07645097374916077], [0.20529937744140625, 0.3914097845554352, 0.4032908082008362], [0.3648030459880829, 0.28974097967147827, 0.34545597434043884], [0.5812397599220276, 0.2535696029663086, 0.1651906818151474], [0.30640149116516113, 0.4527361989021301, 0.24086232483386993], [0.5914095640182495, 0.24153026938438416, 0.16706018149852753], [0.3987728953361511, 0.42567136883735657, 0.17555569112300873], [0.7510204911231995, 0.14296954870224, 0.10601003468036652], [0.3073424696922302, 0.3096867799758911, 0.38297078013420105], [0.34218260645866394, 0.44583970308303833, 0.21197772026062012], [0.7419149279594421, 0.19294926524162292, 0.06513580679893494], [0.545992910861969, 0.20458047091960907, 0.24942660331726074], [0.5713812708854675, 0.3289594054222107, 0.09965939074754715], [0.28301507234573364, 0.43799692392349243, 0.2789880037307739], [0.6001855731010437, 0.2135569155216217, 0.186257466673851], [0.5425904989242554, 0.2989346385002136, 0.158474862575531], [0.15495507419109344, 0.42631128430366516, 0.41873371601104736], [0.8376825451850891, 0.11411532759666443, 0.04820223152637482], [0.3678090572357178, 0.18678107857704163, 0.4454098045825958], [0.5262723565101624, 0.37255582213401794, 0.10117176175117493], [0.5491095781326294, 0.29032081365585327, 0.16056959331035614], [0.15330487489700317, 0.4016721248626709, 0.4450230002403259], [0.38916778564453125, 0.35884523391723633, 0.2519869804382324], [0.41601207852363586, 0.3581587076187134, 0.22582924365997314], [0.2469862550497055, 0.453315407037735, 0.2996983230113983], [0.46657058596611023, 0.3564260005950928, 0.1770033985376358], [0.4986021816730499, 0.3924384117126465, 0.10895942896604538], [0.2521866261959076, 0.4406646192073822, 0.3071487247943878], [0.45424869656562805, 0.3702176809310913, 0.17553356289863586], [0.3957902491092682, 0.4682765603065491, 0.13593316078186035], [0.2829906642436981, 0.40891873836517334, 0.30809056758880615], [0.2369152307510376, 0.4764906167984009, 0.28659412264823914], [0.7008611559867859, 0.19582544267177582, 0.10331334918737411], [0.35785582661628723, 0.4935220777988434, 0.1486220806837082], [0.6816550493240356, 0.1589435636997223, 0.15940141677856445], [0.26990461349487305, 0.48774492740631104, 0.24235042929649353], [0.18236353993415833, 0.41822969913482666, 0.399406760931015], [0.7370857000350952, 0.16648580133914948, 0.09642849862575531], [0.5331408977508545, 0.2989354133605957, 0.16792377829551697], [0.7508445382118225, 0.12268329411745071, 0.1264721006155014], [0.6403642892837524, 0.20562191307544708, 0.15401385724544525], [0.5477755665779114, 0.27352091670036316, 0.1787034571170807], [0.1444023847579956, 0.4171084463596344, 0.4384891986846924], [0.534574568271637, 0.289318710565567, 0.1761067509651184], [0.15146176517009735, 0.3964479863643646, 0.45209020376205444], [0.5842127203941345, 0.34857192635536194, 0.06721541285514832], [0.31065332889556885, 0.3453632891178131, 0.34398332238197327], [0.6700789332389832, 0.2163560837507248, 0.11356503516435623], [0.41564521193504333, 0.4966723322868347, 0.08768250793218613], [0.567184329032898, 0.37575820088386536, 0.05705748125910759], [0.6467160582542419, 0.2746926248073578, 0.07859136164188385], [0.5848965644836426, 0.26319360733032227, 0.15190979838371277], [0.493446946144104, 0.30859988927841187, 0.1979532092809677], [0.6420385241508484, 0.20032578706741333, 0.15763571858406067], [0.4475089907646179, 0.27874600887298584, 0.27374497056007385], [0.3411264419555664, 0.24049542844295502, 0.41837817430496216], [0.3857513666152954, 0.48572230339050293, 0.12852631509304047], [0.43275126814842224, 0.36580801010131836, 0.20144066214561462], [0.6155862808227539, 0.29307821393013, 0.09133544564247131], [0.8764560222625732, 0.0912223532795906, 0.032321635633707047], [0.21414287388324738, 0.43828877806663513, 0.3475683629512787], [0.43110793828964233, 0.36616799235343933, 0.20272405445575714], [0.13948878645896912, 0.26777762174606323, 0.59273362159729], [0.5874183773994446, 0.2819215655326843, 0.1306600421667099], [0.46632590889930725, 0.40497586131095886, 0.1286981999874115], [0.430873304605484, 0.36723941564559937, 0.20188723504543304], [0.549329936504364, 0.277642160654068, 0.17302784323692322], [0.07352537661790848, 0.4079616367816925, 0.5185129046440125], [0.4322509765625, 0.35158324241638184, 0.21616581082344055], [0.39833661913871765, 0.30638983845710754, 0.2952736020088196], [0.17498284578323364, 0.43441179394721985, 0.3906053602695465], [0.38080456852912903, 0.33404627442359924, 0.2851491868495941], [0.8362670540809631, 0.11869508028030396, 0.0450378879904747], [0.057732563465833664, 0.17668478190898895, 0.7655826210975647], [0.18024136126041412, 0.47287502884864807, 0.34688353538513184], [0.7231035232543945, 0.1635400801897049, 0.11335649341344833], [0.2678309679031372, 0.3421131670475006, 0.3900558650493622], [0.33777302503585815, 0.47486263513565063, 0.18736428022384644], [0.34877392649650574, 0.4329570531845093, 0.2182689905166626], [0.19372041523456573, 0.44896429777145386, 0.3573152720928192], [0.5980205535888672, 0.22426241636276245, 0.17771707475185394], [0.12000829726457596, 0.3133619725704193, 0.5666297674179077], [0.758985698223114, 0.1435585469007492, 0.09745580703020096], [0.8120997548103333, 0.11458923667669296, 0.07331099361181259], [0.2275846153497696, 0.3016308844089508, 0.4707845151424408], [0.07762958854436874, 0.24167098104953766, 0.6806994080543518], [0.4315580129623413, 0.33560267090797424, 0.23283931612968445], [0.6560808420181274, 0.2294008582830429, 0.11451821774244308], [0.9670024514198303, 0.022705310955643654, 0.010292302817106247], [0.31103602051734924, 0.4110089838504791, 0.277955025434494], [0.817842960357666, 0.11627192795276642, 0.06588514894247055], [0.3826338052749634, 0.33260026574134827, 0.28476595878601074], [0.4176831841468811, 0.4135369658470154, 0.16877985000610352], [0.9176918268203735, 0.06243732199072838, 0.01987079530954361], [0.13709501922130585, 0.44596391916275024, 0.4169410169124603], [0.39216625690460205, 0.39278683066368103, 0.21504691243171692], [0.34165826439857483, 0.38033610582351685, 0.2780056893825531], [0.816232442855835, 0.14014889299869537, 0.043618686497211456], [0.9193764925003052, 0.06535397469997406, 0.015269529074430466], [0.43856462836265564, 0.35335400700569153, 0.20808134973049164], [0.5125836133956909, 0.30554360151290894, 0.18187279999256134], [0.2598873972892761, 0.3482992947101593, 0.3918133080005646], [0.5141605734825134, 0.3975411057472229, 0.0882982537150383], [0.6343235969543457, 0.2438557893037796, 0.12182064354419708], [0.1041811853647232, 0.3971130847930908, 0.4987057149410248], [0.3919401168823242, 0.36660629510879517, 0.24145358800888062], [0.6262479424476624, 0.2281646728515625, 0.14558738470077515], [0.740488588809967, 0.17692847549915314, 0.08258293569087982], [0.6579723954200745, 0.2395925223827362, 0.10243504494428635], [0.7771086096763611, 0.17242735624313354, 0.050464071333408356], [0.3016991913318634, 0.4577362835407257, 0.2405645102262497], [0.39316365122795105, 0.40244361758232117, 0.20439279079437256], [0.13604511320590973, 0.3944190740585327, 0.46953582763671875], [0.2074100375175476, 0.45963236689567566, 0.33295759558677673], [0.7054973244667053, 0.1609770655632019, 0.13352563977241516], [0.46006983518600464, 0.29829493165016174, 0.24163517355918884], [0.7508055567741394, 0.1738230139017105, 0.07537135481834412], [0.7701060175895691, 0.14471907913684845, 0.08517478406429291], [0.09945504367351532, 0.4732932150363922, 0.42725175619125366], [0.2697422206401825, 0.43180203437805176, 0.2984556555747986], [0.2401140034198761, 0.28794872760772705, 0.47193723917007446], [0.9312824010848999, 0.052464552223682404, 0.01625307835638523], [0.5048120617866516, 0.40984150767326355, 0.08534641563892365], [0.31311163306236267, 0.4361463189125061, 0.2507420778274536], [0.33502396941185, 0.4219134449958801, 0.2430625706911087], [0.9710092544555664, 0.022520137950778008, 0.006470647640526295], [0.9247446060180664, 0.06726745516061783, 0.007987960241734982], [0.5808025002479553, 0.29837584495544434, 0.12082173675298691], [0.6117366552352905, 0.22268109023571014, 0.16558218002319336], [0.6794474720954895, 0.15070350468158722, 0.1698489934206009], [0.16001662611961365, 0.26194286346435547, 0.5780404806137085], [0.27769720554351807, 0.4827451705932617, 0.23955760896205902], [0.4987282454967499, 0.2902269959449768, 0.21104472875595093], [0.7990565896034241, 0.14385683834552765, 0.05708658695220947], [0.7329824566841125, 0.21780630946159363, 0.04921123757958412], [0.2560085356235504, 0.3591725826263428, 0.38481882214546204], [0.22013156116008759, 0.3754604458808899, 0.40440797805786133], [0.41596487164497375, 0.4058816134929657, 0.17815354466438293], [0.3588695526123047, 0.4316580593585968, 0.2094724178314209], [0.39361971616744995, 0.3836948275566101, 0.22268538177013397], [0.37201863527297974, 0.3664512038230896, 0.26153019070625305], [0.27504774928092957, 0.4160865545272827, 0.3088656961917877], [0.3326779901981354, 0.16221372783184052, 0.5051082968711853], [0.31083357334136963, 0.380636990070343, 0.30852946639060974], [0.5255304574966431, 0.31698665022850037, 0.15748289227485657], [0.4880666136741638, 0.2875208854675293, 0.22441250085830688], [0.30107492208480835, 0.4226219058036804, 0.27630314230918884], [0.5075098872184753, 0.2787615954875946, 0.21372853219509125], [0.5540856719017029, 0.3038049340248108, 0.14210936427116394], [0.3257972002029419, 0.3597034215927124, 0.3144994378089905], [0.6041941046714783, 0.2890986502170563, 0.10670732706785202], [0.8679079413414001, 0.07227792590856552, 0.05981416627764702], [0.35207071900367737, 0.4040873348712921, 0.2438420206308365], [0.43422427773475647, 0.32593634724617004, 0.2398393154144287], [0.2670389711856842, 0.40948978066444397, 0.3234712779521942], [0.610419511795044, 0.24421003460884094, 0.14537036418914795], [0.40566712617874146, 0.3469441533088684, 0.24738869071006775], [0.49064892530441284, 0.399501770734787, 0.10984931886196136], [0.19244273006916046, 0.35701343417167664, 0.4505438208580017], [0.5628330111503601, 0.27994561195373535, 0.15722130239009857], [0.9735886454582214, 0.017151296138763428, 0.009260070510208607], [0.4193318486213684, 0.32060766220092773, 0.2600604295730591], [0.2657056152820587, 0.3759535849094391, 0.3583407700061798], [0.2189374417066574, 0.24296823143959045, 0.5380942821502686], [0.7868876457214355, 0.15942606329917908, 0.05368628725409508], [0.6330054998397827, 0.2686798572540283, 0.09831460565328598], [0.37863245606422424, 0.4297391176223755, 0.19162841141223907], [0.433537095785141, 0.3584238290786743, 0.20803914964199066], [0.40368786454200745, 0.4300931394100189, 0.16621902585029602], [0.17627716064453125, 0.5537763833999634, 0.269946426153183], [0.20071066915988922, 0.4004855453968048, 0.39880380034446716], [0.6680976152420044, 0.23471595346927643, 0.09718650579452515], [0.47129109501838684, 0.36310166120529175, 0.1656072586774826], [0.4852190613746643, 0.2891409993171692, 0.22563986480236053], [0.5156071186065674, 0.3843618929386139, 0.10003098845481873], [0.3810421824455261, 0.3399238586425781, 0.279033899307251], [0.2995147705078125, 0.4386654496192932, 0.2618198096752167], [0.36209267377853394, 0.4609711766242981, 0.1769361048936844], [0.28776559233665466, 0.30991917848587036, 0.40231525897979736], [0.4165497124195099, 0.3272917866706848, 0.2561584711074829], [0.6496933698654175, 0.2118007242679596, 0.13850590586662292], [0.2126903086900711, 0.2590264081954956, 0.5282833576202393], [0.5648664236068726, 0.35469335317611694, 0.08044018596410751], [0.561303973197937, 0.28473567962646484, 0.15396033227443695], [0.32140088081359863, 0.4131183922290802, 0.2654806673526764], [0.5201938152313232, 0.3570779263973236, 0.12272825837135315], [0.40003329515457153, 0.3019053339958191, 0.2980613112449646], [0.21630297601222992, 0.20968034863471985, 0.5740166306495667], [0.2041001319885254, 0.3603051006793976, 0.43559470772743225], [0.5390371084213257, 0.2687501609325409, 0.19221265614032745], [0.29775309562683105, 0.4655357599258423, 0.23671108484268188], [0.39159610867500305, 0.4456944763660431, 0.16270941495895386], [0.7276172041893005, 0.16752871870994568, 0.1048540249466896], [0.4554084837436676, 0.3366640508174896, 0.207927405834198], [0.9253494143486023, 0.06416300684213638, 0.010487472638487816], [0.06990550458431244, 0.35652753710746765, 0.5735669732093811], [0.30606192350387573, 0.44427549839019775, 0.2496626377105713], [0.611787736415863, 0.22756201028823853, 0.1606503576040268], [0.07597234845161438, 0.4649079740047455, 0.45911964774131775], [0.1732744574546814, 0.30245137214660645, 0.5242741703987122], [0.6668129563331604, 0.27457162737846375, 0.058615442365407944], [0.2996770143508911, 0.49265292286872864, 0.20767013728618622], [0.3009093105792999, 0.4932164251804352, 0.2058742791414261], [0.40893906354904175, 0.3252415359020233, 0.2658194601535797], [0.08638671040534973, 0.2957693934440613, 0.6178439259529114], [0.5708152055740356, 0.23933051526546478, 0.18985435366630554], [0.3330095112323761, 0.48569396138191223, 0.18129655718803406], [0.4135279059410095, 0.3280848264694214, 0.2583872675895691], [0.47331222891807556, 0.31367525458335876, 0.21301256120204926], [0.08557463437318802, 0.394803524017334, 0.5196217894554138], [0.5130420327186584, 0.37090641260147095, 0.1160515695810318], [0.5214613676071167, 0.3672291934490204, 0.1113094687461853], [0.31002339720726013, 0.4289490282535553, 0.2610275447368622], [0.34402722120285034, 0.39816686511039734, 0.25780588388442993], [0.5013182163238525, 0.28895303606987, 0.20972870290279388], [0.7487872242927551, 0.10180536657571793, 0.14940743148326874], [0.2921542823314667, 0.4470164477825165, 0.26082929968833923], [0.6245514750480652, 0.2836507558822632, 0.09179776161909103], [0.7117139101028442, 0.1676429808139801, 0.12064314633607864], [0.5152862668037415, 0.36098796129226685, 0.12372579425573349], [0.6769434213638306, 0.254657119512558, 0.06839944422245026], [0.31716272234916687, 0.3893515169620514, 0.29348576068878174], [0.7099737524986267, 0.20505015552043915, 0.08497611433267593], [0.24948324263095856, 0.3271782100200653, 0.42333856225013733], [0.2205311357975006, 0.33705615997314453, 0.44241276383399963], [0.5087262392044067, 0.16988950967788696, 0.3213842213153839], [0.05955111235380173, 0.3207901418209076, 0.6196587681770325], [0.27603039145469666, 0.470210462808609, 0.25375911593437195], [0.47378870844841003, 0.29108190536499023, 0.23512940108776093], [0.5890395641326904, 0.16453419625759125, 0.24642625451087952], [0.3502020835876465, 0.3265126049518585, 0.32328537106513977], [0.2735636532306671, 0.35533660650253296, 0.37109971046447754], [0.18478621542453766, 0.3330959677696228, 0.48211777210235596], [0.17256665229797363, 0.49834632873535156, 0.3290870189666748], [0.19831188023090363, 0.5035038590431213, 0.29818427562713623], [0.5565930604934692, 0.2427474409341812, 0.20065948367118835], [0.7048251628875732, 0.1520647555589676, 0.14311009645462036], [0.662104070186615, 0.2334405928850174, 0.10445533692836761], [0.31447723507881165, 0.3951278030872345, 0.29039502143859863], [0.398336797952652, 0.2452617585659027, 0.3564014732837677], [0.3262537717819214, 0.3832075595855713, 0.2905386686325073], [0.30896177887916565, 0.49275198578834534, 0.19828620553016663], [0.5574311017990112, 0.22583912312984467, 0.2167297899723053], [0.5947386026382446, 0.3451389968395233, 0.060122404247522354], [0.2847917079925537, 0.4789009988307953, 0.2363073229789734], [0.7252503633499146, 0.21415917575359344, 0.060590505599975586], [0.6800176501274109, 0.18533112108707428, 0.13465124368667603], [0.29109182953834534, 0.4666205644607544, 0.24228762090206146], [0.6021477580070496, 0.3163805305957794, 0.0814717710018158], [0.42524006962776184, 0.2913398742675781, 0.28342005610466003], [0.5574547648429871, 0.2975916564464569, 0.1449536830186844], [0.13412363827228546, 0.3019009232521057, 0.5639755129814148], [0.2759568691253662, 0.2992970049381256, 0.42474618554115295], [0.3926181197166443, 0.3291926681995392, 0.27818918228149414], [0.5086315870285034, 0.2942085266113281, 0.19715997576713562], [0.4562724828720093, 0.3584021031856537, 0.18532544374465942], [0.5330228209495544, 0.2012808471918106, 0.26569631695747375], [0.10552789270877838, 0.31644555926322937, 0.578026533126831], [0.6705410480499268, 0.16084297001361847, 0.16861596703529358], [0.9828511476516724, 0.014047831296920776, 0.0031009474769234657], [0.5058892369270325, 0.2837234437465668, 0.21038728952407837], [0.125861257314682, 0.40926653146743774, 0.4648721516132355], [0.33086246252059937, 0.3918089270591736, 0.2773285508155823], [0.3370514214038849, 0.3083980083465576, 0.35455048084259033], [0.07309321314096451, 0.48611980676651, 0.44078701734542847], [0.43364498019218445, 0.40316012501716614, 0.163194939494133], [0.3898909389972687, 0.4366993308067322, 0.17340974509716034], [0.5227606892585754, 0.29813364148139954, 0.17910566926002502], [0.622440755367279, 0.2542332112789154, 0.12332608550786972], [0.2545306086540222, 0.4132310748100281, 0.3322383463382721], [0.7270249128341675, 0.22283817827701569, 0.05013694986701012], [0.5949164628982544, 0.2736423909664154, 0.13144122064113617], [0.3078327476978302, 0.4376436471939087, 0.2545236647129059], [0.36034074425697327, 0.39983126521110535, 0.239827960729599], [0.580786406993866, 0.2827647626399994, 0.13644880056381226], [0.4360913634300232, 0.32684361934661865, 0.23706498742103577], [0.34324130415916443, 0.25861161947250366, 0.3981470465660095], [0.869597315788269, 0.08613095432519913, 0.044271670281887054], [0.5503681898117065, 0.22002802789211273, 0.22960375249385834], [0.7753952145576477, 0.16175363957881927, 0.06285110116004944], [0.19343788921833038, 0.37678849697113037, 0.42977359890937805], [0.5202725529670715, 0.30642691254615784, 0.1733005791902542], [0.20524682104587555, 0.420834481716156, 0.3739188015460968], [0.3565375506877899, 0.41493573784828186, 0.2285267859697342], [0.2725750207901001, 0.4041902720928192, 0.3232347071170807], [0.38689857721328735, 0.2588127553462982, 0.3542886972427368], [0.38095492124557495, 0.3912024199962616, 0.22784264385700226], [0.6638785004615784, 0.2071010321378708, 0.12902048230171204], [0.5965346097946167, 0.3119320273399353, 0.09153341501951218], [0.15232354402542114, 0.46197837591171265, 0.3856981098651886], [0.4334540367126465, 0.3572828471660614, 0.20926308631896973], [0.6690120697021484, 0.29391375184059143, 0.03707408905029297], [0.6530129313468933, 0.2464573085308075, 0.10052976757287979], [0.4520519971847534, 0.3782685399055481, 0.1696794033050537], [0.2471158355474472, 0.6456291079521179, 0.10725511610507965], [0.45349758863449097, 0.3505654036998749, 0.19593693315982819], [0.2502976357936859, 0.332978755235672, 0.4167236089706421], [0.41460296511650085, 0.45535048842430115, 0.1300465166568756], [0.35415029525756836, 0.4036577641963959, 0.24219195544719696], [0.3270490765571594, 0.3676648736000061, 0.30528607964515686], [0.3035929501056671, 0.538193941116333, 0.15821316838264465], [0.5973577499389648, 0.2884560525417328, 0.11418620496988297], [0.32292401790618896, 0.3737275302410126, 0.30334845185279846], [0.5856255888938904, 0.2446024864912033, 0.1697719246149063], [0.5235720872879028, 0.27952826023101807, 0.19689960777759552], [0.3994763493537903, 0.33987173438072205, 0.26065191626548767], [0.6940289735794067, 0.23278003931045532, 0.07319097220897675], [0.5619260668754578, 0.29553553462028503, 0.1425384134054184], [0.24372661113739014, 0.4295015335083008, 0.3267718255519867], [0.35566291213035583, 0.4625152051448822, 0.18182185292243958], [0.5892910957336426, 0.3078630268573761, 0.10284590721130371], [0.16355088353157043, 0.28834688663482666, 0.5481022000312805], [0.3338015377521515, 0.4287235736846924, 0.23747484385967255], [0.5870580673217773, 0.24329635500907898, 0.1696454882621765], [0.20884135365486145, 0.5864841341972351, 0.20467452704906464], [0.6671544313430786, 0.2347588837146759, 0.0980866551399231], [0.3863238990306854, 0.3397466540336609, 0.2739294767379761], [0.44832661747932434, 0.2966928482055664, 0.25498056411743164], [0.5686917901039124, 0.27386415004730225, 0.1574440598487854], [0.41046375036239624, 0.3383990526199341, 0.25113722681999207], [0.9206153154373169, 0.0574977844953537, 0.021886970847845078], [0.44925013184547424, 0.3140002489089966, 0.2367495745420456], [0.44265496730804443, 0.29979902505874634, 0.2575460374355316], [0.584514319896698, 0.2810966968536377, 0.13438890874385834], [0.32694950699806213, 0.38106104731559753, 0.29198941588401794], [0.586328387260437, 0.2673981487751007, 0.1462734490633011], [0.7837032079696655, 0.1815853863954544, 0.03471139445900917], [0.6100850105285645, 0.3325721323490143, 0.057342853397130966], [0.2738615870475769, 0.5137979984283447, 0.21234050393104553], [0.9472562670707703, 0.035544585436582565, 0.017199072986841202], [0.4115177392959595, 0.3726731240749359, 0.21580907702445984], [0.32395222783088684, 0.4248366057872772, 0.2512111961841583], [0.2804836928844452, 0.3542254865169525, 0.3652908205986023], [0.6026002168655396, 0.2189932018518448, 0.17840655148029327], [0.6703076362609863, 0.21163517236709595, 0.11805715411901474], [0.6359270811080933, 0.26681020855903625, 0.09726268798112869], [0.6283115148544312, 0.20113787055015564, 0.1705506592988968], [0.4435845613479614, 0.3218337595462799, 0.23458172380924225], [0.22071175277233124, 0.336910605430603, 0.44237762689590454], [0.4348825216293335, 0.34030041098594666, 0.22481711208820343], [0.7718712687492371, 0.19753509759902954, 0.030593616887927055], [0.33068183064460754, 0.3908860981464386, 0.2784320116043091], [0.5616689920425415, 0.2756141722202301, 0.16271689534187317], [0.36883869767189026, 0.366882860660553, 0.264278382062912], [0.5980609655380249, 0.31175583600997925, 0.09018325805664062], [0.666887640953064, 0.1940130740404129, 0.1390993595123291], [0.16082946956157684, 0.4087248146533966, 0.43044573068618774], [0.41104334592819214, 0.4416055381298065, 0.14735108613967896], [0.19776847958564758, 0.42945411801338196, 0.37277740240097046], [0.6255834698677063, 0.2401094287633896, 0.13430708646774292], [0.6831790804862976, 0.25542864203453064, 0.06139231100678444], [0.10577026754617691, 0.192953959107399, 0.7012757658958435], [0.19511273503303528, 0.4524123966693878, 0.3524748980998993], [0.34646108746528625, 0.3756164610385895, 0.27792248129844666], [0.4054933488368988, 0.2718947231769562, 0.3226119875907898], [0.20520099997520447, 0.4174342155456543, 0.37736472487449646], [0.5067525506019592, 0.3746321499347687, 0.1186152845621109], [0.5358391404151917, 0.2798842489719391, 0.18427665531635284], [0.7837457656860352, 0.10402603447437286, 0.11222825199365616], [0.0951400101184845, 0.21629276871681213, 0.6885672211647034], [0.6452211737632751, 0.2500954270362854, 0.10468345135450363], [0.418001264333725, 0.3870968222618103, 0.19490188360214233], [0.5847263932228088, 0.20257031917572021, 0.21270327270030975], [0.40747562050819397, 0.3354114294052124, 0.25711295008659363], [0.4928179979324341, 0.2010599821805954, 0.30612194538116455], [0.3713316023349762, 0.2652723789215088, 0.3633960485458374], [0.7179995179176331, 0.19555272161960602, 0.08644776046276093], [0.6744864583015442, 0.26666584610939026, 0.05884774029254913], [0.3617123067378998, 0.5070455074310303, 0.13124211132526398], [0.39944979548454285, 0.492404043674469, 0.10814619809389114], [0.1386021375656128, 0.5902688503265381, 0.27112898230552673], [0.6158831119537354, 0.2540898025035858, 0.13002710044384003], [0.3166736364364624, 0.40717771649360657, 0.27614858746528625], [0.5153043866157532, 0.30317825078964233, 0.1815173178911209], [0.7779892086982727, 0.14589913189411163, 0.07611162960529327], [0.23296168446540833, 0.4130488336086273, 0.35398948192596436], [0.29152911901474, 0.42294737696647644, 0.28552353382110596], [0.535452663898468, 0.29189059138298035, 0.1726568192243576], [0.7671207189559937, 0.16954649984836578, 0.06333277374505997], [0.7836337685585022, 0.1564279943704605, 0.059938330203294754], [0.23816479742527008, 0.44037729501724243, 0.3214578330516815], [0.5735434889793396, 0.30067479610443115, 0.12578175961971283], [0.4280371069908142, 0.3911248445510864, 0.1808379739522934], [0.46579447388648987, 0.3312743008136749, 0.20293129980564117], [0.41254082322120667, 0.4128095209598541, 0.17464961111545563], [0.3952600061893463, 0.31803470849990845, 0.2867053151130676], [0.3369992673397064, 0.43606677651405334, 0.22693392634391785], [0.562222421169281, 0.3445652425289154, 0.0932123064994812], [0.6413575410842896, 0.25576379895210266, 0.10287855565547943], [0.24032393097877502, 0.3848418891429901, 0.3748341202735901], [0.3201999366283417, 0.38790810108184814, 0.29189199209213257], [0.6837368607521057, 0.26791608333587646, 0.04834704473614693], [0.42903056740760803, 0.42368143796920776, 0.14728794991970062], [0.597134530544281, 0.25910550355911255, 0.14375996589660645], [0.33309516310691833, 0.4504137337207794, 0.21649113297462463], [0.2054222673177719, 0.33164891600608826, 0.4629288613796234], [0.10639107972383499, 0.3419904112815857, 0.5516185164451599], [0.15468347072601318, 0.44422438740730286, 0.4010921120643616]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPWLsHZ1sUuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_label = []\n",
        "for i in range(len(preds_)):\n",
        "  if preds_[i] == [1, 0, 0]:\n",
        "    pred = 'NAG'\n",
        "  elif preds_[i] == [0, 1, 0]:\n",
        "    pred = 'CAG'\n",
        "  elif preds_[i]== [0, 0, 1]:\n",
        "    pred = 'OAG'\n",
        "  test_label.append(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCZsiQiX08Fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "with open('test_prediction.csv', 'w') as csvfile:\n",
        "    fieldnames = ['ID', 'Label']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=['ID', 'Label'])\n",
        "\n",
        "    writer.writeheader()\n",
        "    for i in range(len(test_set[0])):\n",
        "      writer.writerow({'ID': test_set[0][i], 'Label': test_label[i]})\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak9qmAjL3Nkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"test_prediction.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}